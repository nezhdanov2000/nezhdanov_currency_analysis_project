<h1>Анализ рынка валют</h1> 

Задачей проекта является создание ETL-процесса для формирования данных, необходимых для анализа изменений курсов акций на рынке. Для этого мы будем использовать данные о курсах акций компаний, которые будут загружаться с использованием API Alpha Vantage (https://www.alphavantage). Затем мы создадим витрину данных, включающую следующие поля:

Уникальный идентификатор категории.
Название компании (биржевой тикер).
Общий объем торгов за последние сутки.
Курс на момент открытия торгов за день.
Курс на момент закрытия торгов за день.
Процентное изменение курса от открытия до закрытия.
Минимальный временной интервал, на котором был зафиксирован наибольший объем торгов.
Минимальный временной интервал, на котором был зафиксирован наивысший курс за день.
Минимальный временной интервал, на котором был зафиксирован наименьший курс за день.
Загрузка данных будет осуществляться в двух режимах:

Инициализирующий – загрузка полного объема данных источника.
Инкрементальный – загрузка изменений за прошедшие сутки.
Структура хранения данных включает:

Сырой слой данных.
Промежуточный слой.
Слой витрин.
Для выполнения проекта используется следующий стек технологий:

Вместо Spark используется SQL из-за небольшого объема данных и сжатых сроков.
Для хранения бэкапа сырых данных используется обычная файловая система и CSV файлы.
В качестве оркестратора выбран Airflow из-за удобства просмотра логов, возможности управления задачами и условиями.
Хранилище данных - Postgres, добавленный в Docker контейнер Airflow.

Инициализация проекта включает следующие шаги:

Задание параметров для Postgres, API и списка тикеров компаний.
Создание папки для хранения CSV файлов и предоставление прав на запись в неё.
Запуск Docker контейнеров и ожидание запуска Airflow.
Создание переменных Airflow.
Копирование скриптов DAG'ов в папку Docker/dags.
Загрузка и обработка данных выполняются в Airflow с помощью двух DAG'ов:

Полная (инициализирующая) загрузка данных.
Инкрементальная загрузка данных каждые сутки.
Данные полученные по API сохраняются в CSV файлы и в слой сырых данных в Postgres. Структура данных остается простой, и вместо нормализации данных используются материализованные представления для каждого тикера в промежуточном слое.

Проект также предусматривает возможные улучшения и развитие, такие как хранение бэкапов в HDFS, использование Spark для обработки данных, разработка веб-интерфейса для настройки параметров процесса ETL и выбора тикеров, а также добавление визуализации метрик качества данных и процесса.

---------------------------------------------------------------
<h1>Инициализация Проекта:</h1> 


Для начального запуска Docker-контейнеров и настройки Airflow используется скрипт execute.sh, который выполняет следующие шаги:

Устанавливает параметры для Postgres, API и список тикеров компаний (переменные заданы в начале и могут быть изменены при необходимости).

Создает каталог "data" для хранения CSV файлов и устанавливает права на запись в него.

Запускает и настраивает Docker-контейнеры согласно файлу docker/docker-compose.yaml.

Ожидает, пока Airflow будет готов к работе.

Создает переменные для Airflow.

Копирует скрипты DAG'ов в каталог docker/dags.

Для загрузки и обработки данных используется Airflow, и задачи выполняются следующим образом:

Полная (инициализирующая) загрузка данных - DAG initial_load.py:

Создает базу данных (create_database).
Загружает данные через API и сохраняет их в формате CSV и в Postgres (load_data).
Создает промежуточный слой (create_views).
Формирует итоговую витрину данных (create_mart).
Инкрементальная загрузка, выполняемая каждый день - DAG incremental_load.py:

Загружает данные за прошедшие сутки (get_daily_data).
Обновляет промежуточный слой (refresh_views).
Обновляет витрину данных (refresh_mart).
Хранение данных и построение витрины:

Данные, полученные через API, сохраняются "как есть" в виде CSV файлов (в качестве резервной копии) и в сыром слое данных в Postgres, где каждая таблица соответствует определенному тикеру. При записи данных в Postgres, дублирующиеся значения перезаписываются.

Поскольку структура данных проста и не содержит общих или дублирующихся полей, не была выполнена нормализация данных или их объединение в одну или несколько таблиц. Вместо этого для каждого тикера (компании) в промежуточном слое создаются материализованные представления, в которых предварительно вычисляются вспомогательные значения для построения итоговой витрины. Материализованные представления выбраны из-за следующих преимуществ:

Не требуется дополнительное место для хранения данных.
Обеспечивается быстрый доступ к данным (результаты запросов кэшируются).
Простота обновления (вручную или автоматически при обновлении исходных таблиц).
То же самое применяется и для построения итоговой витрины данных: временные витрины создаются для каждого тикера в виде CTE-запросов, а затем объединяются с помощью UNION в одну общую витрину. SQL-скрипты для Postgres генерируются в DAG'ах Airflow с использованием шаблонов, и значения из списка тикеров подставляются в цикле.

Нереализованный функционал:

Не реализован контроль качества данных, сбор статистики и обработка ошибок (включая исключения, сбои при загрузке и ошибки сохранения на диск).

Улучшения и развитие проекта:

Возможные улучшения и доработки включают:

Хранение бэкапов данных в HDFS для повышения надежности и скорости.
Использование Apache Spark при увеличении объема данных или добавлении витрин, чтобы обеспечить более эффективную обработку данных.
Разработка веб-интерфейса для настройки параметров ETL-процесса и выбора тикеров.
Добавление визуализации (dashboard) для мониторинга качества данных, процесса выполнения и результатов витрины.
